{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1680d4b-49a6-4c8e-9a63-dba8c0d920a2",
   "metadata": {},
   "source": [
    "## LAB 5 (Not compolsory) ##  \n",
    "### Connecting Spark to Snowflake managed Iceberg tables without Polaris Catalog ###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37df6598-a793-4ec1-b5bb-8bd1fc7aaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f383a1fc-643a-4cd7-be35-2cb24c3415bc",
   "metadata": {},
   "source": [
    "In the next cell you need to replace `<user_env>` with the directory of your anaconda environment. This allows the Jupyter notebook to find your spark environment.\n",
    "It should look something like: `'/Users/mstarc/anaconda3/envs/iceberg-lab-techup/lib/python3.12/site-packages/pyspark'` when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8a473-6a7a-4b37-9627-0694ffe48d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = '<user_env>/lib/python3.12/site-packages/pyspark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa2dd9f6-be83-47d2-836c-a077787a3aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mstarc/anaconda3/envs/iceberg-lab-techup2/lib/python3.12/site-packages/pyspark'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9a2b8a-0992-48c3-b587-947f99d63127",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d6df2d-57ed-42ad-b63a-cff413f21c78",
   "metadata": {},
   "source": [
    "In the next cell you need to replace `<private_key_location>` with the location of your private key. It should look something like: `/Users/mstarc/.ssh/rsa_key.p8` when done.  You will also need to replace `<username>` with your CAS2 username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7472aae4-9776-4eba-9872-e7701c73bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SNOWFLAKE_CATALOG_URI'] = \"jdbc:snowflake://sfpscogs-aws_cas2.snowflakecomputing.com\"\n",
    "os.environ['SNOWFLAKE_ROLE'] = \"ACCOUNTADMIN\"\n",
    "os.environ['SNOWFLAKE_USERNAME'] = \"<USERNAME>\"\n",
    "os.environ['SNOWFLAKE_PASSWORD'] = \"<private_key_location>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbbe7c-0bef-429d-8ff2-5a46ad83e01a",
   "metadata": {},
   "source": [
    "There is no need to change any of the values in the below cells. I have provided the AWS credentials to simplify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcab866-9cd3-4914-b2db-c001ed1c2e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PACKAGES'] = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1,net.snowflake:snowflake-jdbc:3.14.2,software.amazon.awssdk:bundle:2.20.160,software.amazon.awssdk:url-connection-client:2.20.160\"\n",
    "os.environ['AWS_REGION'] = \"us-west-2\"\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"***REMOVED***R\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"***REMOVED***\"\n",
    "# If you are using session credentials instead of simple name/secret credentials, also set the environment variable below\n",
    "#os.environ['AWS_SESSION_TOKEN'] = \"<your aws session token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce62921-cc0d-41aa-b3d5-04f640f88f0a",
   "metadata": {},
   "source": [
    "# Run Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc30a594-eaec-47e5-9b37-1420936ee943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20a99ce0-e4a4-4e52-89f3-cb6e3e4665f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/mstarc/anaconda3/envs/iceberg-lab-techup2/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/mstarc/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mstarc/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "net.snowflake#snowflake-jdbc added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f3937dce-aa33-4c9b-9a14-2ed99d78ee12;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 in central\n",
      "\tfound net.snowflake#snowflake-jdbc;3.14.2 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.160 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.160 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.160 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.160 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.160 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.160 in central\n",
      ":: resolution report :: resolve 232ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tnet.snowflake#snowflake-jdbc;3.14.2 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.1 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.160 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.160 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.160 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.160 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.160 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.160 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f3937dce-aa33-4c9b-9a14-2ed99d78ee12\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/5ms)\n",
      "24/10/03 15:37:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/03 15:37:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession, for AWS\n",
    "spark = SparkSession.builder.appName('iceberg_lab')\\\n",
    "    .config('spark.jars.packages', os.environ['PACKAGES'])\\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5e5a01-0ab7-493e-909e-e5dac884a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/03 15:37:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession, specifically for GCP where GCS needs the shaded jar as described here:\n",
    "# https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md#troubleshooting-the-installation\n",
    "spark = SparkSession.builder.appName('iceberg_lab')\\\n",
    "    .config('spark.jars', 'https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.18/gcs-connector-hadoop3-2.2.18-shaded.jar')\\\n",
    "    .config('spark.jars.packages', os.environ['PACKAGES'])\\\n",
    "    .config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c160774-3d56-4e2f-9102-7b745da4a12f",
   "metadata": {},
   "source": [
    "### Spark configurations for all clouds\n",
    "Regardless which cloud your Snowflake account is in, set the following configurations for Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad480e1-8a79-4d81-a5ae-9f6401d5545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.defaultCatalog\", \"snowflake_catalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog.catalog-impl\", \"org.apache.iceberg.snowflake.SnowflakeCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog.uri\", os.environ['SNOWFLAKE_CATALOG_URI'])\n",
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog.jdbc.role\", os.environ['SNOWFLAKE_ROLE'])\n",
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog.jdbc.user\", os.environ['SNOWFLAKE_USERNAME'])\n",
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog.jdbc.private_key_file\", os.environ['SNOWFLAKE_PASSWORD'])\n",
    "spark.conf.set(\"spark.sql.iceberg.vectorization.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b759ea-1e59-4dee-ae52-c558a3e7f0ff",
   "metadata": {},
   "source": [
    "### Spark configurations for AWS\n",
    "If your Snowflake account and object storage are on AWS, set these additional Spark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b430d4-bbbe-464d-be18-4184b2404dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.catalog.snowflake_catalog.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "# If you are using session credentials instead of simple name/secret credentials, use the credentials provider configuration below instead of the line above below\n",
    "#spark.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "# If you are using session credentials instead of simple name/secret credentials, also set the configuration below\n",
    "#spark.conf.set(\"spark.hadoop.fs.s3a.session.token\", os.environ['AWS_SESSION_TOKEN'])\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.endpoint.region\", os.environ['AWS_REGION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5304667-f991-4004-8385-0a948f1bc007",
   "metadata": {},
   "source": [
    "# Read Snowflake-managed Iceberg Tables #   \n",
    "You will also need to replace `<username>` with your CAS2 username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ea146b-8a09-4459-ab59-6008c6f1766c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|namespace                |\n",
      "+-------------------------+\n",
      "|MSTARC.INFORMATION_SCHEMA|\n",
      "|MSTARC.LAB3              |\n",
      "|MSTARC.MSTARC            |\n",
      "|MSTARC.PUBLIC            |\n",
      "|MSTARC.STEP              |\n",
      "|MSTARC.STEP2             |\n",
      "|MSTARC.STEP3             |\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW NAMESPACES IN <username>\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c4f77-eba1-466d-b790-ffe989d768e6",
   "metadata": {},
   "source": [
    "Replace `<namespace>` with the value from the previous step relating to the namespace you created in LAB3. It should be like `MSTARC.LAB3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f02e0a2-eacc-4627-8016-46b45613c3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE <namespae>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f411a03-97b1-417a-8feb-632c70b15b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------+\n",
      "|namespace  |tableName         |isTemporary|\n",
      "+-----------+------------------+-----------+\n",
      "|MSTARC.LAB3|TEST_TABLE_MANAGED|false      |\n",
      "+-----------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7733e-77a5-4692-bcc2-0a670cb979fd",
   "metadata": {},
   "source": [
    "Replace `<snowflake managed table>` with `namespace.tableName` from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adf6b944-ca4f-4c08-9d0c-56931d951875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---\n",
      " COL1 | 3   \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"<snowflake managed table>\")\n",
    "df.show(20, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a558b3-aa64-44fd-a935-8cea55c419fa",
   "metadata": {},
   "source": [
    "Once you seeyour record from the Snowflake managed Iceberg table the lab is complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
