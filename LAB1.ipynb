{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf6f65e-96a4-4da7-abf5-473f602eb3cb",
   "metadata": {},
   "source": [
    "## LAB 1 ##  \n",
    "\n",
    "This lab will see you connect from pyspark to Polaris catalog, creating a namespace and a table managed by Polaris\n",
    "\n",
    "In the next cell you need to replace <user_env> with the directory of your anaconda environment. This allows the Jupyter notebook to find your spark environment.\n",
    "It should look something like: `'/Users/mstarc/anaconda3/envs/iceberg-lab-techup/lib/python3.12/site-packages/pyspark'` when done.\n",
    "\n",
    "You will also need to replace `credential` below. It will use the Polaris connection details you previously created in the form `<CLIENT ID>:<SECRET ID>`. It should look like: `EobogpKvVdz3STi1XP/R15oeyM8=:YSP51zJhSIFelzPyat5GGP74mVBdEfht7e71Cc2wlfk=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58fdf83-eaee-4cd4-9075-5f5a2befc36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/01 22:09:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   mstarc|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '<user_env>/lib/python3.12/site-packages/pyspark'\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('iceberg_lab') \\\n",
    ".config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.1,software.amazon.awssdk:bundle:2.20.160,software.amazon.awssdk:url-connection-client:2.20.160') \\\n",
    ".config('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    ".config('spark.sql.defaultCatalog', 'polaris') \\\n",
    ".config('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.polaris.type', 'rest') \\\n",
    ".config('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation','vended-credentials') \\\n",
    ".config('spark.sql.catalog.polaris.uri','https://tzb93977.snowflakecomputing.com/polaris/api/catalog') \\\n",
    ".config('spark.sql.catalog.polaris.credential','<CLIENT ID>:<SECRET>') \\\n",
    ".config('spark.sql.catalog.polaris.warehouse','apj_ps_tmup_int') \\\n",
    ".config('spark.sql.catalog.polaris.scope','PRINCIPAL_ROLE:admin_int') \\\n",
    ".config('spark.sql.catalog.polaris.client.region','us-west-2') \\\n",
    ".getOrCreate()\n",
    "\n",
    "#Show namespaces\n",
    "spark.sql(\"show namespaces\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e55565-a2aa-4edd-98ac-734c98eaef13",
   "metadata": {},
   "source": [
    "In the below cells replace `<username>` with your CAS2 login name or first part of your snowflake email (remove full stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8585a0-a4f5-463a-9cb3-f3d05e44f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create namespace\n",
    "spark.sql(\"create namespace <username>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146a55e-fce5-454a-ab44-af69c564a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use namespace\n",
    "spark.sql(\"use namespace <username>\")\n",
    "\n",
    "#Show tables; this will show no tables since it is a new namespace\n",
    "spark.sql(\"show tables\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61586f1b-a6f7-4e9e-b207-397e9cb08b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a test table\n",
    "spark.sql(\"create table apj_ps_managed (col1 int) using iceberg\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988ce79-cc53-4287-a15f-f9e2733244b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert a record in the table\n",
    "spark.sql(\"insert into apj_ps_managed values (1)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3b533-ee84-4667-be6e-7c8b822851ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#query the table\n",
    "spark.sql(\"select * from apj_ps_managed\").show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62b9d1-7bfb-4963-9f6e-72edf4975bf4",
   "metadata": {},
   "source": [
    "Once you can see the data in your created table. Go back to Polaris and refresh the catalogs. You should see your namespace and newly created table. You can then move onto LAB2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
